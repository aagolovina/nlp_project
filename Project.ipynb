{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "94d23897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743d1c9",
   "metadata": {},
   "source": [
    "# Предобработка\n",
    "\n",
    "Размер корпуса - более 2 миллионов словоупотреблений.\n",
    "\n",
    "В качестве источника текстов выбрала сайт [lib.ru](http://www.lib.ru), который позволяет скачать txt-версии необходимых произведений. Удаление нетекстовых элементов происходило вручную (поскольку краулером это сделать было просто невозможно, к тому же, если сайт предоставляет возможность получить txt-файлы, кажется резонным воспользоваться такой возможностью):\n",
    "1. Удалялись пометы об авторской орфографии и т.п.\n",
    "2. В некоторых файлах символ \"J\" заменялся на \"ё\"\n",
    "3. Удалялась жанровая принадлежность\n",
    "4. Удалялись номера страниц в фигурных скобках\n",
    "\n",
    "Сначала был создан словарь соответствий произведений ссылкам (ключ - название txt-файла, значение - ссылка на произведение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a35ed3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_for_file = {\n",
    "    \"ivandenisych.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/ivandenisych.txt\",\n",
    "    \"matren.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/matren.txt\",\n",
    "    \"na_izlomah.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/na_izlomah.txt\",\n",
    "    \"r_dvarasskaza.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/r_dvarasskaza.txt\",\n",
    "    \"r_hod.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/r_hod.txt\",\n",
    "    \"r_kochet.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/r_kochet.txt\",\n",
    "    \"rk.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/rk.txt\",\n",
    "    \"sol_gv.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/sol_gv.txt\",\n",
    "    \"solg_as.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/solg_as.txt\",\n",
    "    \"vkp1.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/vkp1.txt\",\n",
    "    \"vkp2.txt\": \"http://www.lib.ru/PROZA/SOLZHENICYN/vkp2.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671b46d",
   "metadata": {},
   "source": [
    "Далее собираю список словарей, содержащих информацию о произведении (автор, название, текст и ссылка, взятая по названию файла из словаря в предыдущей ячейке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4212434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = []\n",
    "\n",
    "for book in os.listdir(r\"project\"):\n",
    "    with open(\n",
    "        os.path.join(r\"project\", book), \"r\", encoding=\"ANSI\"\n",
    "    ) as f:\n",
    "        text_info = {}\n",
    "        file = f.read()\n",
    "        text_info[\"author\"] = file.split(\"\\n\")[0].split(\". \")[0]\n",
    "        text_info[\"name\"] = file.split(\"\\n\")[0].split(\". \")[1]\n",
    "        text_info[\"text\"] = sent_tokenize(\"\\n\".join(file.split(\"\\n\")[1:]))\n",
    "        text_info[\"source\"] = url_for_file[book]\n",
    "        all_info.append(text_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6e647",
   "metadata": {},
   "source": [
    "# Морфологический анализ\n",
    "\n",
    "Изначально хотела использовать в качестве парсера stanza, но в библиотеке не оказалось русских лемм, поэтому пришлось поменять на spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ca41b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7743a",
   "metadata": {},
   "source": [
    "Функция токенизации необходима для того, чтобы запрос выполнялся без учета пунктуации (вряд ли кто-либо будет искать n-граммы, включая в запрос запятые, да и пунктуационная разметка не входила в задачи написания корпуса). Токенизироваться будет каждое предложение при записи в ту таблицу базы данных, где будут храниться словоформы, их леммы и грамматические теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5b84840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \"\"\"\n",
    "    Функция для токенизации текстов\n",
    "    Аргумент - строка с текстом отзыва\n",
    "    Вывод - список токенов\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r\"[а-я0-9ё]+\")\n",
    "    text_tokenized = tokenizer.tokenize(text)\n",
    "    return \" \".join(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912271df",
   "metadata": {},
   "source": [
    "Функция записи в базу данных будет заполнять сразу все три таблицы базы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "82dd17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(block):\n",
    "    \"\"\"\n",
    "    Функция для записи информации о произведении в базу данных\n",
    "    Аргумент - блок-словарь с мета-информацией и текстом произведения\n",
    "    \"\"\"\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO meta_info \n",
    "            (book_title, author, source) VALUES (?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "            block[\"name\"],\n",
    "            block[\"author\"],\n",
    "            block[\"source\"]\n",
    "            )\n",
    "    )\n",
    "    conn.commit()\n",
    "    \n",
    "    meta = cur.execute(\"\"\"SELECT meta_id FROM meta_info\"\"\").fetchone()[0]\n",
    "    \n",
    "    for sentence in block[\"text\"]:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO sentences\n",
    "                (meta_id, sentence) VALUES (?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                meta,\n",
    "                sentence\n",
    "            ),\n",
    "        )\n",
    "        conn.commit()\n",
    "        \n",
    "    cur.execute(\"SELECT sentence_id, sentence FROM sentences\")\n",
    "    \n",
    "    for sent_id, sent_text in cur.fetchall():\n",
    "        doc = nlp(tokenization(sent_text))\n",
    "        spacy_parse = [\n",
    "            tuple([str(token.text), str(token.lemma_), str(token.pos_)])\n",
    "            for token in doc\n",
    "        ]\n",
    "        \n",
    "        for parse in spacy_parse:        \n",
    "            cur.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO parsing\n",
    "                    (sentence_id, word, lemma, pos) VALUES (?, ?, ?, ?)\n",
    "                    \"\"\",\n",
    "                    (\n",
    "                        sent_id,\n",
    "                        parse[0],\n",
    "                        parse[1],\n",
    "                        parse[2]\n",
    "                    ),\n",
    "                )\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d9c43",
   "metadata": {},
   "source": [
    "Устанавливаем соединение с базой данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "54326554",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"solgenitsin.db\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd2f2",
   "metadata": {},
   "source": [
    "Создаем три таблицы:\n",
    "\n",
    "1. Мета-информация о произведении (название, автор, источник-ссылка)\n",
    "2. Предложения (айди произведения, из которого взято предложение, и само предложение)\n",
    "3. Морфологическая информация (айди предложения, из которого взято слово, само слово, лемма, грамматический тег)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "608a3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS meta_info (\n",
    "    meta_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    book_title TEXT,\n",
    "    author TEXT,\n",
    "    source TEXT\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sentences (\n",
    "    sentence_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    meta_id INTEGER,\n",
    "    sentence TEXT\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS parsing (\n",
    "    word_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    sentence_id INTEGER,\n",
    "    word TEXT,\n",
    "    lemma TEXT,\n",
    "    pos TEXT\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d55203",
   "metadata": {},
   "source": [
    "Дальше проходимся по списку словарей с информацией по произведениям, составленному ранее, и мучительно долго записываем все в базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f335aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 11/11 [7:24:21<00:00, 2423.80s/it]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(all_info):\n",
    "    write_to_db(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce45f9",
   "metadata": {},
   "source": [
    "# Функция поиска\n",
    "\n",
    "Сначала необходимо выделить все грамматические теги, которые встречаются в разметке (для дальнейшего использования в функции поиска)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c5c4990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRON', 'ADP', 'VERB', 'AUX', 'CCONJ', 'DET', 'PUNCT', 'NOUN', 'NUM', 'X', 'ADV', 'SYM', 'PROPN', 'PART', 'SCONJ', 'INTJ', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "tags_query = \"\"\"SELECT pos FROM parsing\"\"\"\n",
    "cur.execute(tags_query)\n",
    "print(set([tag[0] for tag in cur.fetchall()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da726817",
   "metadata": {},
   "source": [
    "Далее идет функция поиска, которая получает на вход запрос, делит его по пробелам. Проходясь по каждому компоненту запроса, функция делает соответствующий запрос в базу данных и записывает в общий список соответствий списки соответствий по каждому компоненту (в списках находятся кортежи с айди предложения и айди совпавшего слова). Далее происходит уточнение, n-граммы какого типа мы ищем. Наконец, в случае биграмм и триграмм идет перебор всех кортежей в первом списке (т.е. соответствий первому компоненту запроса) с проверкой наличия элементов на расстоянии 1 (и в случае триграмм и их последнего елемента, на расстоянии 2) в предложении с таким же айди, то есть проверяется есть ли в следующих списках кортежи, в которых айди слова отличается на 1 (или 2) от айди первого слова, а айди предложения совпадают. Айди подходящих предложений записываются в отдельный список. По этим айди происходит получение необходимой мета-информации, которую функция возвращает в виде списка кортежей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "646280d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    \"\"\"\n",
    "    Функция для поиска соответствий заданному запросу\n",
    "    Аргумент - запрос\n",
    "    Вывод - список кортежей с текстом и мета-информацией каждого подходящего предложения\n",
    "    \"\"\"\n",
    "    queries_dict = {\n",
    "        'wordform': \"\"\"SELECT word_id, sentence_id FROM parsing WHERE word=?\"\"\",\n",
    "        'lemma_and_pos': \"\"\"SELECT word_id, sentence_id FROM parsing WHERE lemma=? AND pos=?\"\"\",\n",
    "        'pos': \"\"\"SELECT word_id, sentence_id FROM parsing WHERE pos=?\"\"\",\n",
    "        'lemma': \"\"\"SELECT word_id, sentence_id FROM parsing WHERE lemma=?\"\"\"\n",
    "    }\n",
    "        \n",
    "    tags = {'PRON', 'ADP', 'VERB', 'AUX', 'CCONJ', 'DET', 'PUNCT', 'NOUN', 'NUM', 'X', 'ADV', 'SYM', 'PROPN', 'PART', 'SCONJ', 'INTJ', 'ADJ'}\n",
    "    \n",
    "    query_res = [] # список, в который помещаются списки соответствий каждому компоненту запроса\n",
    "    \n",
    "    ngramm = query.split(\" \")\n",
    "\n",
    "    for element in ngramm:\n",
    "        if re.match(r'\".+\"', element) is not None:\n",
    "            element = element[1:-1]\n",
    "            cur.execute(queries_dict['wordform'], (element, ))\n",
    "        elif \"+\" in element:\n",
    "            cur.execute(queries_dict['lemma_and_pos'], tuple(element.split(\"+\")))\n",
    "        elif element in tags:\n",
    "            cur.execute(queries_dict['pos'], (element, ))\n",
    "        else:\n",
    "            doc = nlp(element)\n",
    "            lemma = [token.lemma_ for token in doc][0]\n",
    "            cur.execute(queries_dict['lemma'], (lemma, ))\n",
    "        query_res.append(cur.fetchall())\n",
    "    \n",
    "    golden = [] # список айди подходящих предложений\n",
    "    \n",
    "    if len(query_res) == len(ngramm): # если найдены соответствия всем компонентам n-граммы\n",
    "        if len(ngramm) == 1: # если поиск униграммы\n",
    "            for res in query_res:\n",
    "                for item in res:\n",
    "                    golden.append(item[1])\n",
    "        elif len(ngramm) == 2: # если поиск биграммы\n",
    "            for item in query_res[0]:\n",
    "                if tuple([item[0] + 1, item[1]]) in query_res[1]:\n",
    "                    golden.append(item[1])\n",
    "        elif len(ngramm) == 3: # если поиск триграммы\n",
    "            for item in query_res[0]:\n",
    "                if tuple([item[0] + 1, item[1]]) in query_res[1] and tuple([item[0] + 2, item[1]]) in query_res[2]:\n",
    "                    golden.append(item[1])\n",
    "                \n",
    "    golden_tuples = []\n",
    "                \n",
    "    for golden_id in golden:\n",
    "        cur.execute(\"\"\"SELECT sentence, book_title, author, source FROM sentences JOIN meta_info ON sentences.meta_id = meta_info.meta_id WHERE sentence_id=?\"\"\", (golden_id, ))\n",
    "        golden_tuples.append(cur.fetchone())\n",
    "        \n",
    "    return golden_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c165587",
   "metadata": {},
   "source": [
    "# Тестирование\n",
    "\n",
    "На данный момент выводы ячеек очищены от предыдущих тестов. Поскольку объем корпуса достаточно большой, запись осуществляется в генерируемый файл (и происходит достаточно долго).\n",
    "\n",
    "**Примеры запросов:**\n",
    "\n",
    "Униграммы:\n",
    "1. NUM\n",
    "2. было+VERB (либо, напротив, следующий запрос: было+AUX)\n",
    "3. \"ступая\"\n",
    "\n",
    "Биграммы:\n",
    "1. без NOUN\n",
    "2. рассматривающий+ADJ NOUN\n",
    "3. далее+ADV VERB\n",
    "\n",
    "Триграммы:\n",
    "1. NOUN и NOUN\n",
    "2. PRON VERB на\n",
    "3. не VERB NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = '' # сюда добавить запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(req)\n",
    "\n",
    "if len(results) == 0:\n",
    "    print('К сожалению, по Вашему запросу ничего не найдено. Проверьте, что Вы не забыли про букву \"ё\", или попробуйте ввести другой запрос.')\n",
    "else:\n",
    "    with open(\"results.txt\", \"w\", encoding=\"utf_8\") as f:\n",
    "        for item in results:\n",
    "            f.write(f'{item[0]}\\nИсточник: {item[2]} \"{item[1]}\" (доступно по ссылке: {item[3]} )\\n---\\n')\n",
    "    print(\"По Вашему запросу был создан файл с выдачей.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06167f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
